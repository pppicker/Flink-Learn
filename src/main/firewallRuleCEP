package com.example


//import Utils.PropertiesUtils
import com.alibaba.fastjson.{JSON, JSONObject}
import com.example.utils.{IPRangeChecker, PropertyUtils}
import org.apache.flink.cep.pattern.conditions.SimpleCondition
//import dynamicrule.JDBCTest.getStringDb
import org.apache.flink.api.common.serialization.SimpleStringSchema
import org.apache.flink.cep.PatternSelectFunction
import org.apache.flink.cep.scala.CEP
import org.apache.flink.cep.scala.pattern.Pattern
import org.apache.flink.streaming.api.TimeCharacteristic
import org.apache.flink.streaming.api.scala.{DataStream, StreamExecutionEnvironment, createTypeInformation}
import org.apache.flink.streaming.api.windowing.time.Time
import org.apache.flink.streaming.connectors.kafka.{FlinkKafkaConsumer, FlinkKafkaProducer}
import org.slf4j.{Logger, LoggerFactory}
import java.util.Properties
import java.util


// 定义样例类，Event用来存放输入数据，Warning存放输出数据
case class Event(srcIP: String, dstIP: String, dstPort: String, proto: String)
case class Warning(srcIP: List[String], dstIP: List[String], dstPort: String, proto: String, ruleId: String, rulePrimaryKey: String)

//class Warning(var name: String, var age: Int)
//class Warning(srcIP: List[String], dstIP: List[String], dstPort: String, proto: String, var ruleId: String, var rulePrimaryKey: String)

/**
 * @author xyh
 * @date 2023/11/3 20:15
 **/


object FirewallRuleCEP {


  def main(args: Array[String]): Unit = {

    val logger: Logger = LoggerFactory.getLogger(this.getClass)

    //初始化规则id
    var ruleId = ""
    var groupIdFirewallRule = ""
    if (args.length > 0) {
      for (i <- 0 to args.length - 1) {
        logger.info("i==", i)
        logger.info("args==", args(i))
        ruleId = args(0).split("@")(0)
        //groupid根据@将传入数据分割得到
        groupIdFirewallRule = args(0).split("@")(1)
      }
    }
    logger.info("flink_ruleId=", ruleId)
    logger.info("flink_group_id_firewall_dr=", groupIdFirewallRule)
    //读取数据库里的参数配置
//    val rules = getStringDb(ruleId)
    val rules = ""
    //data_count:100,threatPort:12121,threatProtocol:17,tags:异常登录,interval_time:0,threat_level:high
    val formatRule = rules.replace(",", ":")
    logger.info("formatRule: " + formatRule)
    //将规则数据分割为数组，方便后面获取使用
    val ruleArray = formatRule.split(":")

    //开始获取传入的规则相关信息



    //日志统计计数
    val dataCount = ruleArray(1).toInt
    //威胁端口
    val threatPort = ruleArray(3).toInt
    //协议
    val threatProtocol = ruleArray(5).toInt
    //标签
    val tags = ruleArray(7)
    //间隔时间
    val intervalTime = ruleArray(9).toInt
    //威胁等级
    val threatLevel = ruleArray(11)
    //规则id
    val ruleId = ruleArray(13)
    //数据源类型
    val dataSource = ruleArray(15)
    //规则主键
    val rulePrimaryKey = ruleArray(17)
    //sourceIP 支持范围与独立IP
    val sourceIPRange = ruleArray(19)
    //dstIP 支持范围与独立IP
    val dstIPRange = ruleArray(21)


    val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment
    //配置任务并行度
    env.setParallelism(4)
    //使用处理时间语义
    env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime)

    //kafka的配置
    val properties = new Properties()
    properties.setProperty("bootstrap.servers", PropertyUtils.getProp("kafka.bootstrap.servers"))
    //设置kafka消费者groupId
    properties.setProperty("group.id", s"consume_id_firewall_${ruleId}_${groupIdFirewallRule}")


    val kafkaSource = new FlinkKafkaConsumer[String]("bigdata-firewall-one-part-topic", new SimpleStringSchema(), properties)
    //创建DataStream
    val sourceStream: DataStream[String] = env.addSource(kafkaSource)

    //将kafka数据中message原始字符串解析为k=v格式
    def str2kv(x: String): String = {
      //按Json格式解析日志，获取message字段信息
      var strMessage: String = JSON.parseObject(x).getString("message")

      strMessage = strMessage.replace(": ", ":")
      //删除message信息中的双引号
      strMessage = strMessage.replace("\"", "")

      //匹配多个空格进行分割
      val strArray: Array[String] = strMessage.split("\\s+")

      //初始化结果
      var buf = ""

      //遍历array,解析为key=value，key2=value2格式
      for (i <- 0 to (strArray.length - 1)) {
        val keyValueArray = strArray(i).split("=")
        var tempkV = ""
        for (j <- 0 to (keyValueArray.length - 1)) {
          tempkV += "" + keyValueArray(j) + "" + "="
          if (j == 1)
            tempkV = tempkV.dropRight(1)
        }
        buf += tempkV + ","
      }
      buf = buf.dropRight(1)
      buf = buf.replace("=", ":")
      buf
    }

    //将原始日志流数据解析为kv格式
    val kvStream = sourceStream.map(x => str2kv(x))

    //将数据转换为Event对象
    def str2Event(x: String): Event = {

      val hashMap = new java.util.HashMap[String, String]
      val elementArray = x.split(",")
      for (j <- 0 to (elementArray.length - 1)) {
        if (elementArray(j).split(":").length >= 2) {
          //json.put(elementArray(j).split(":")(0), elementArray(j).split(":")(1))
          if ((elementArray(j).split(":")(0) != "") && (elementArray(j).substring(elementArray(j).indexOf(":") + 1, elementArray(j).length) != "")) {
            hashMap.put(elementArray(j).split(":")(0), elementArray(j).substring(elementArray(j).indexOf(":") + 1, elementArray(j).length))
          }
        }
      }
      //判断字段值是否缺失
      if (hashMap.containsKey("srcip") && hashMap.containsKey("dstip") && hashMap.containsKey("dstport") && hashMap.containsKey("proto")) {

        Event(hashMap.get("srcip"),hashMap.get("dstip"),hashMap.get("dstport"),hashMap.get("proto"))
      }
      else {
        //字段值缺失统一置为空值
        Event("","","","")
      }
    }

    //过滤掉字段值缺失的数据
    val eventStream = kvStream.map(x => str2Event(x)).filter(x => !x.srcIP.equals(""))


    //创建Pattern匹配规则，根据srcIP,dstIP,dstPort,proto,dataCount条件对数据进行匹配
    val pattern = Pattern.begin[Event]("start")
      .where(event => IPRangeChecker.isIPInRange(event.srcIP,sourceIPRange) && IPRangeChecker.isIPInRange(event.dstIP,dstIPRange) && event.dstPort == threatPort && event.proto.equals(threatProtocol))
      .times(dataCount)
      .within(Time.seconds(intervalTime))

    //对数据流应用匹配规则
    val patternStream = CEP.pattern(eventStream, pattern)

    // 使用select方法来捕捉结果，EventMatch为捕捉结果的函数
    val resultStream = patternStream.select(new EventMatch(threatPort, threatProtocol, ruleId ,rulePrimaryKey))

    val kafkaProperties = new Properties

    kafkaProperties.setProperty("bootstrap.servers", "10.1.178.67:9092,10.1.178.68:9092,10.1.178.69:9092")

    val myProducer = new FlinkKafkaProducer[String](
      "threat-device",
      new SimpleStringSchema(),
      kafkaProperties
    )
    resultStream.addSink(myProducer)

    env.execute("firewall-rule-cep")

  }
}

//重写PatternSelectFunction方法，用Warning样例类来接收数据
class EventMatch(dstPort: String, proto: String, ruleId: String ,rulePrimaryKey: String ) extends PatternSelectFunction[Event, Warning] {
  override def select(map: util.Map[String, util.List[Event]]): Warning = {
    val srcIPList: List[String] = List()
    val dstIPList: List[String] = List()
    //遍历获取所有匹配到的event，将信息加入到List中
    map.get("start").forEach(event => srcIPList :+ event.srcIP)
    map.get("start").forEach(event => dstIPList :+ event.srcIP)

    //proto，ruleId，rulePrimaryKey字段由方法外面传入赋值
    Warning(srcIPList, dstIPList, dstPort, proto, ruleId, rulePrimaryKey)
  }
}
